{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsVg6TBFzala"
      },
      "source": [
        "# WGAN-GP\n",
        "\n",
        "**W**asserstein **G**enterative **A**dversarial **N**etwork - **G**radient **P**enalty\n",
        "\n",
        "with the CelebA Face Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGv82jIBzr74"
      },
      "source": [
        "### Hardware Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7qJc9gnzXJQ",
        "outputId": "8c1ea43e-640e-452a-bbc0-9b9ae8190d5b"
      },
      "outputs": [],
      "source": [
        "def get_hardware_info(use_in_notebook=True, install_packages=True):\n",
        "    import platform\n",
        "    system_name = platform.system()\n",
        "\n",
        "    if install_packages:\n",
        "        if system_name.lower() == \"windows\":\n",
        "            %pip install psutil    # or: conda install psutil\n",
        "            %pip install gputil\n",
        "            %pip install py-cpuinfo\n",
        "        elif system_name.lower() == \"linux\":\n",
        "            !pip install psutil    # or: conda install psutil\n",
        "            !pip install gputil\n",
        "            !pip install py-cpuinfo\n",
        "\n",
        "    # import needed packages\n",
        "    import psutil\n",
        "    import GPUtil\n",
        "    from cpuinfo import get_cpu_info\n",
        "\n",
        "    if use_in_notebook:\n",
        "        if install_packages:\n",
        "            if system_name.lower() == \"windows\":\n",
        "                %pip install ipython\n",
        "            elif system_name.lower() == \"linux\":\n",
        "                !pip install ipython\n",
        "\n",
        "        from IPython.display import clear_output\n",
        "        clear_output()\n",
        "\n",
        "    print(\"-\"*32, \"\\nYour Hardware:\\n\")\n",
        "\n",
        "    # General\n",
        "    print(\"    ---> General <---\")\n",
        "    print(\"Operatingsystem:\", platform.system())\n",
        "    print(\"Version:\", platform.version())\n",
        "    print(\"Architecture:\", platform.architecture())\n",
        "    print(\"Processor:\", platform.processor())\n",
        "\n",
        "    # GPU-Information\n",
        "    print(\"\\n    ---> GPU <---\")\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    for gpu in gpus:\n",
        "        print(\"GPU Name:\", gpu.name)\n",
        "        print(\"VRAM Total:\", gpu.memoryTotal, \"MB\")\n",
        "        print(\"VRAM Used:\", gpu.memoryUsed, \"MB\")\n",
        "        print(\"Utilization:\", gpu.load * 100, \"%\")\n",
        "\n",
        "    # CPU-Information\n",
        "    print(\"\\n    ---> CPU <---\")\n",
        "    cpu_info = get_cpu_info()\n",
        "    print(\"CPU-Name:\", cpu_info[\"brand_raw\"])\n",
        "    print(\"CPU Kernels:\", psutil.cpu_count(logical=False))\n",
        "    print(\"Logical CPU-Kernels:\", psutil.cpu_count(logical=True))\n",
        "    print(\"CPU-Frequence:\", psutil.cpu_freq().max, \"MHz\")\n",
        "    print(\"CPU-Utilization:\", psutil.cpu_percent(interval=1), \"%\")\n",
        "\n",
        "    # RAM-Information\n",
        "    print(\"\\n    ---> RAM <---\")\n",
        "    ram = psutil.virtual_memory()\n",
        "    print(\"RAM Total:\", ram.total // (1024**3), \"GB\")\n",
        "    print(\"RAM Available:\", ram.available // (1024**3), \"GB\")\n",
        "    print(\"RAM-Utilization:\", ram.percent, \"%\")\n",
        "\n",
        "    print(f\"\\n{'-'*32}\")\n",
        "\n",
        "\n",
        "get_hardware_info(use_in_notebook=True, install_packages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j1U7F2rzx2I"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVh-WzoZzzJx",
        "outputId": "20e0eed0-a134-40bf-ce3f-485309382b1c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt_style = 'seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else np.random.choice(plt.style.available)\n",
        "plt.style.use(plt_style)\n",
        "print(f\"Using '{plt_style}''\")\n",
        "import cv2\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cx9El8Je0GXV"
      },
      "outputs": [],
      "source": [
        "def imshow(img, title=None, image_width=10, axis=False,\n",
        "           color_space=\"RGB\", cols=1, save_to=None,\n",
        "           hspace=0.2, wspace=0.2,\n",
        "           use_original_sytle=False, invert=False):\n",
        "    \"\"\"\n",
        "    Visualizes one or multiple images.\n",
        "\n",
        "    Image will be reshaped: [batch_size/images, width, height, channels]\n",
        "\n",
        "    title can be None, str or a list of strings.\n",
        "    \"\"\"\n",
        "    import cv2\n",
        "\n",
        "    original_style = plt.rcParams.copy()\n",
        "\n",
        "    img_shape = img.shape\n",
        "    print(f\"Got images with shape: {img_shape}\")\n",
        "\n",
        "    # tranform the image to the right form\n",
        "    if len(img_shape) == 2:\n",
        "        img = np.reshape(img, shape=(1, img.shape[0], img.shape[1], 1))\n",
        "    elif len(img_shape) == 3:\n",
        "        # check if multiple gray images or multiple images with channel\n",
        "        if img.shape[2] < img.shape[0] and img.shape[1] == img.shape[2]:\n",
        "            img = np.reshape(img, shape=(1, img.shape[0], img.shape[1], img.shape[3]))\n",
        "        else:\n",
        "            # there could be cases where this is wrong\n",
        "            img = np.reshape(img, shape=(img.shape[0], img.shape[1], img.shape[3], 1))\n",
        "        img = np.reshape(img, shape=(1, img.shape[0], img.shape[1], 1))\n",
        "    elif len(img_shape) != 4:\n",
        "        raise ValueError(f\"Image(s) have wrong shape! Founded shape: {img.shape}.\")\n",
        "\n",
        "    print(f\"Transformed shape to: {img_shape}\")\n",
        "\n",
        "    # invert images\n",
        "    if invert:\n",
        "        print(\"Invert images...\")\n",
        "        max_value = 2**(img.dtype.itemsize * 8) -1\n",
        "        scaling_func = lambda x: max_value - x\n",
        "        img = np.apply_along_axis(scaling_func, axis=0, arr=img)\n",
        "\n",
        "    # Set visualization settings\n",
        "    # aspect_ratio_width = img.shape[1] / img.shape[2]\n",
        "    aspect_ratio = img.shape[2] / img.shape[1]\n",
        "\n",
        "    n_images = img.shape[0]\n",
        "    rows = n_images//cols + int(n_images % cols > 0)\n",
        "\n",
        "    width = int(image_width * cols)\n",
        "    height = int(image_width * rows * aspect_ratio)\n",
        "\n",
        "    # set plt style\n",
        "    if not use_original_sytle:\n",
        "        plt_style = 'seaborn-v0_8' if 'seaborn-v0_8' in plt.style.available else np.random.choice(plt.style.available)\n",
        "        plt.style.use(plt_style)\n",
        "        print(f\"Using '{plt_style}'' plotting style.\")\n",
        "\n",
        "    # plotting\n",
        "    print(f\"Making you a beautiful plot...\")\n",
        "    fig, ax = plt.subplots(nrows=rows, ncols=cols, figsize=(width, height))\n",
        "    ax = ax.ravel()\n",
        "    fig.subplots_adjust(hspace=hspace, wspace=wspace)\n",
        "    if type(title) == str:\n",
        "        fig.suptitle(title, fontsize=128, y=0.95)\n",
        "\n",
        "    for idx in range(len(ax)):\n",
        "        cur_ax = ax[idx]\n",
        "\n",
        "        if idx >= len(img):\n",
        "            cur_ax.axis(\"off\")\n",
        "            continue\n",
        "\n",
        "        cur_img = img[idx]\n",
        "\n",
        "        if color_space.lower() == \"bgr\":\n",
        "            cur_img = cv2.cvtColor(cur_img, cv2.COLOR_BGR2RGB)\n",
        "            cmap = None\n",
        "        elif color_space.lower() == \"rgb\":\n",
        "            cur_img = cur_img\n",
        "            cmap = None\n",
        "        elif color_space.lower() == \"hsv\":\n",
        "            cur_img = cv2.cvtColor(cur_img, cv2.COLOR_HSV2RGB)\n",
        "            cmap = None\n",
        "        elif color_space.lower() in [\"gray\", \"grey\", \"g\"]:\n",
        "            cur_img = cur_img\n",
        "            cmap = \"gray\"\n",
        "\n",
        "        cur_ax.imshow(cur_img, cmap=cmap)\n",
        "\n",
        "        if type(title) in [list, tuple]:\n",
        "            cur_ax.set_title(title[idx], fontsize=64)\n",
        "        if axis == False:\n",
        "            cur_ax.axis(\"off\")\n",
        "\n",
        "    if save_to:\n",
        "        os.makedirs(os.path.split(save_to)[0], exist_ok=True)\n",
        "        fig.savefig(save_to, dpi=300)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    if not use_original_sytle:\n",
        "        # reset to original plt style\n",
        "        plt.rcParams.update(original_style)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "_Dy4Y7Xp0JIw",
        "outputId": "f2c25d1e-f419-408d-ce90-ea638cd579d0"
      },
      "outputs": [],
      "source": [
        "def get_cur_date_time_as_str():\n",
        "    now = datetime.now()\n",
        "    return f\"{now.year:04}-{now.month:02}-{now.day:02}_{now.hour:02}-{now.minute:02}-{now.second:02}\"\n",
        "\n",
        "get_cur_date_time_as_str()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMxzA85kLj7N"
      },
      "outputs": [],
      "source": [
        "def get_used_depth(img:np.ndarray):\n",
        "    \"\"\"\n",
        "    Find the number of possible/used pixel values in an image.\n",
        "\n",
        "    Only works if the pixel space is really used!\n",
        "    \"\"\"\n",
        "    max_value = img.max()\n",
        "    if max_value - 1 <= 0:\n",
        "        return 1\n",
        "    elif max_value - 2**8-1 <= 0:\n",
        "        return 8\n",
        "    elif max_value - 2**16-1 <= 0:\n",
        "        return 16\n",
        "    elif max_value - 2**32-1 <= 0:\n",
        "        return 32\n",
        "    else:\n",
        "        raise ValueError(\"The depth of the given image is not sure:\", max_value)\n",
        "\n",
        "def get_depth(img:np.ndarray):\n",
        "    \"\"\"\n",
        "    Returns the depth of an image in bit.\n",
        "    \"\"\"\n",
        "    return img.dtype.itemsize * 8\n",
        "\n",
        "def change_bit_depth_with_scaling(image, new_bit_depth=None):\n",
        "    old_dtype = image.dtype\n",
        "    int_types = {8: np.uint8, 16: np.uint16, 32: np.uint32, 64: np.uint64}\n",
        "    float_types = {16: np.float16, 32: np.float32, 64: np.float64}\n",
        "\n",
        "    # old depth pixel space\n",
        "    old_bit_depth = old_dtype.itemsize * 8\n",
        "    old_min, old_max = (0, 2**old_bit_depth - 1) if np.issubdtype(old_dtype, np.integer) else (0.0, 1.0)\n",
        "    print(f\"Old bit depth: {old_bit_depth} bit\")\n",
        "\n",
        "    # new datatype\n",
        "    if new_bit_depth is None:\n",
        "        new_bit_depth = get_used_depth(image)\n",
        "        print(f\"Found a used depth space of {new_bit_depth} bit\")\n",
        "\n",
        "    if np.issubdtype(old_dtype, np.integer):\n",
        "        new_dtype = int_types.get(new_bit_depth, None)\n",
        "        new_min, new_max = 0, 2**new_bit_depth - 1\n",
        "    elif np.issubdtype(old_dtype, np.floating):\n",
        "        new_dtype = float_types.get(new_bit_depth, None)\n",
        "        new_min, new_max = 0.0, 1.0\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported dtype\")\n",
        "\n",
        "    if new_dtype is None:\n",
        "        raise ValueError(f\"Unsupported bit depth: {new_bit_depth}\")\n",
        "\n",
        "    # scaling and applying\n",
        "    if new_dtype == old_dtype:\n",
        "        print(\"No datatyp change done! But dat got scaled\")\n",
        "    else:\n",
        "        print(f\"Change and scaled from {old_dtype} ({old_bit_depth} bit) -> {new_dtype} ({new_bit_depth} bit)\")\n",
        "\n",
        "    norm_array = (image.astype(np.float32) - old_min) / (old_max - old_min)\n",
        "    scaled_array = norm_array * (new_max - new_min) + new_min\n",
        "\n",
        "    return scaled_array.astype(new_dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZD5FaL3r0Ur0"
      },
      "outputs": [],
      "source": [
        "EXPERIMENT_NAME = get_cur_date_time_as_str() + \"_DCGAN\"\n",
        "LOG_DIR = \"./logs/fit/\" + get_cur_date_time_as_str()\n",
        "\n",
        "LATENT_SPACE_DIMS = 32\n",
        "IMAGE_SIZE = 64\n",
        "CHANNELS = 3\n",
        "EPOCHS = 50\n",
        "BATCH_SIZE = 128*2\n",
        "LEARNING_RATE_DIS = 5e-5\n",
        "LEARNING_RATE_GEN = 2e-4\n",
        "CRITIC_STEPS = 3\n",
        "GP_WEIGHT = 10.0\n",
        "\n",
        "\n",
        "os.makedirs(f\"./logs\", exist_ok=True)\n",
        "os.makedirs(f\"./checkpoints\", exist_ok=True)\n",
        "os.makedirs(f\"./models/{EXPERIMENT_NAME}\", exist_ok=True)\n",
        "os.makedirs(LOG_DIR, exist_ok=True)\n",
        "os.makedirs(\"./output/\", exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnpcnkiWz1_3"
      },
      "source": [
        "### Data Loading & Preprocessing\n",
        "\n",
        "CelebA Face Dataset\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "**Kaggle API token loading**\n",
        "1. Go to https://www.kaggle.com/ and create or sign in your account\n",
        "2. Click on your profile picture > Settings and go to API\n",
        "3. Click on 'Create New Token' and the 'kaggle.json' fie should download automatically\n",
        "4. Now continue here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "id": "1FMqb-bE1H0b",
        "outputId": "e100952d-2763-47c4-905e-50432003fe29"
      },
      "outputs": [],
      "source": [
        "import platform\n",
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "def load_kaggle_dataset(author_name, dataset_name, on_google_colab,\n",
        "                        download_path, goal_path,\n",
        "                        kaggle_local_path=\"./\", kaggle_file_name=\"kaggle.json\"):\n",
        "\n",
        "    # variables\n",
        "    print(\"Set some variables...\")\n",
        "    dataset_download_name = f\"{author_name}/{dataset_name}\"\n",
        "\n",
        "    zip_file_name = f\"{dataset_name}.zip\"\n",
        "    zip_file_download_path = os.path.join(download_path, zip_file_name)\n",
        "\n",
        "    kaggle_file_cur_path = os.path.join(kaggle_local_path, kaggle_file_name)\n",
        "    kaggle_goal_path = os.path.expanduser(\"~/.kaggle\") if platform.system().lower() == \"windows\" else \"/root/.config/kaggle\"\n",
        "    kaggle_goal_file_path = os.path.join(kaggle_goal_path, kaggle_file_name)\n",
        "\n",
        "    # make sure that the goal path exist\n",
        "    os.makedirs(goal_path, exist_ok=True)\n",
        "    os.makedirs(kaggle_goal_path, exist_ok=True)\n",
        "\n",
        "    print(\"Finding and placing the API file...\")\n",
        "    # upload in google colab\n",
        "    if on_google_colab:\n",
        "        kaggle_local_path = \"./\"\n",
        "        kaggle_file_cur_path = os.path.join(kaggle_local_path, kaggle_file_name)\n",
        "        if os.path.exists(kaggle_file_cur_path):\n",
        "            os.remove(kaggle_file_cur_path)\n",
        "\n",
        "        from google.colab import files\n",
        "        files.upload()  # choose your local 'kaggle.json' file\n",
        "\n",
        "    # get the kaggle API file to the right spot\n",
        "    if os.path.exists(kaggle_goal_file_path):\n",
        "        os.remove(kaggle_goal_file_path)\n",
        "    shutil.copy2(kaggle_file_cur_path, kaggle_goal_path)\n",
        "    os.chmod(kaggle_goal_file_path, 600)    # set right rights\n",
        "    print(f\"Cpopied to: {kaggle_goal_path}\")\n",
        "\n",
        "    # init Kaggle API\n",
        "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "    print(\"Autheticating at Kaggle API...\")\n",
        "    api = KaggleApi()\n",
        "    api.authenticate()\n",
        "\n",
        "    # make sure the file not exist already\n",
        "    if os.path.exists(zip_file_download_path):\n",
        "        os.remove(zip_file_download_path)\n",
        "\n",
        "    # download kaggle dataset\n",
        "    print(\"Downloading dataset...\")\n",
        "    #    -> dataset name just in the https link the last 2 items\n",
        "    # !kaggle datasets download -d joosthazelzet/lego-brick-images\n",
        "    api.dataset_download_files(dataset_download_name, path=download_path, unzip=False)\n",
        "\n",
        "    # Unzip the downloaded dataset\n",
        "    print(\"Unzipping dataset...\")\n",
        "    if os.path.exists(goal_path):\n",
        "        shutil.rmtree(goal_path)\n",
        "    # !unzip -q \"lego-brick-images.zip\" -d dataset\n",
        "    with zipfile.ZipFile(zip_file_download_path, \"r\") as zip_ref:\n",
        "        zip_ref.extractall(goal_path)\n",
        "\n",
        "    # delete zip file\n",
        "    os.remove(zip_file_download_path)\n",
        "\n",
        "    print(f\"Congratulations! Downloaded successfull '{dataset_name}' from '{author_name}' ðŸ¥³ðŸ˜Ž\")\n",
        "\n",
        "\n",
        "\n",
        "load_kaggle_dataset(\n",
        "    author_name=\"jessicali9530\",\n",
        "    dataset_name=\"celeba-dataset\",\n",
        "    on_google_colab=True,\n",
        "    download_path=\"/content/\",\n",
        "    goal_path=f\"/content/dataset\",\n",
        "    kaggle_local_path=\"./\",\n",
        "    kaggle_file_name=\"kaggle.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UtI2d5zlz2XI",
        "outputId": "f3b0a3c5-c2fd-4b92-e5dc-933ab3e8b3fc"
      },
      "outputs": [],
      "source": [
        "train_data = tf.keras.utils.image_dataset_from_directory(\n",
        "    \"./dataset/img_align_celeba/img_align_celeba\",\n",
        "    labels=None,\n",
        "    color_mode=\"rgb\",\n",
        "    image_size=(IMAGE_SIZE,IMAGE_SIZE),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    interpolation=\"bilinear\"\n",
        ")\n",
        "\n",
        "example_image = next(iter(train_data.take(1))).numpy()\n",
        "print(\"Batches:\", train_data.cardinality().numpy())\n",
        "print(\"Batch-Size:\", example_image.nbytes / (1024 ** 3), \"GB\")\n",
        "print(\"Files:\", len(train_data.file_paths))\n",
        "print(\"Shape:\", example_image.shape)    # creating iter object and get the first output\n",
        "print(\"Min:\", round(example_image.min(), 0), \"Max:\", round(example_image.max(), 0))\n",
        "print(f\"Pixel depth: {get_depth(example_image)} bit, Used pixel depth: {get_used_depth(example_image)}bit\")\n",
        "print(train_data.element_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAVmDtuILmvN",
        "outputId": "55e86ccb-e11a-43c2-ed6c-ed2d5d61ac8b"
      },
      "outputs": [],
      "source": [
        "def preprocessing(image):\n",
        "    # scaling around -1, 1\n",
        "    max_value = 255\n",
        "    shifting_scaling_factor = max_value/2\n",
        "    image = (tf.cast(image, \"float32\") -shifting_scaling_factor) / shifting_scaling_factor\n",
        "    return image\n",
        "\n",
        "train_data = train_data.map(lambda x: preprocessing(x))\n",
        "\n",
        "# get shape from last batch\n",
        "data_gen = iter(train_data.take(len(train_data)))\n",
        "for _ in range(len(train_data)):\n",
        "    cur_img = next(data_gen)\n",
        "\n",
        "last_batch_size = cur_img.numpy().shape[0]\n",
        "\n",
        "example_image = next(iter(train_data.take(1))).numpy()\n",
        "print(\"Batches:\", train_data.cardinality().numpy())\n",
        "print(\"Batch-Size:\", example_image.nbytes / (1024 ** 3), \"GB\")\n",
        "print(\"Files:\", (len(train_data)-1)*example_image.shape[0]+last_batch_size)\n",
        "print(\"Shape:\", example_image.shape)    # creating iter object and get the first output\n",
        "print(\"Min:\", round(example_image.min(), 2), \"Max:\", round(example_image.max(), 2))\n",
        "print(f\"Pixel depth: {get_depth(example_image)} bit, Used pixel depth: {get_used_depth(example_image)}bit\")\n",
        "print(train_data.element_spec)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vJr5fmimCymX",
        "outputId": "8cb83c8a-e764-4a83-e84f-860e925e0a14"
      },
      "outputs": [],
      "source": [
        "data_gen = iter(train_data)\n",
        "max_samples = 50\n",
        "sampled = 0\n",
        "samples = None\n",
        "\n",
        "while sampled < max_samples:\n",
        "    cur_data = next(data_gen)\n",
        "    for cur_img in cur_data[:]:\n",
        "        if np.random.random() > 0.8:\n",
        "            sampled += 1\n",
        "            if samples is not None:\n",
        "                samples = np.concatenate((samples, np.reshape(cur_img, (1,)+cur_img.shape)), axis=0)\n",
        "            else:\n",
        "                samples = np.reshape(cur_img, (1,)+cur_img.shape)\n",
        "\n",
        "            if sampled >= max_samples:\n",
        "                break\n",
        "\n",
        "print(samples.shape)\n",
        "imshow(samples, cols=4, color_space=\"gray\", invert=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMFcnTGYWF_4"
      },
      "source": [
        "### Build Critic (Discriminator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNYaPxzvWJPN"
      },
      "outputs": [],
      "source": [
        "critic_input = layers.Input(shape=(IMAGE_SIZE, IMAGE_SIZE, CHANNELS))\n",
        "\n",
        "x = layers.Conv2D(IMAGE_SIZE, kernel_size=4, strides=2, padding=\"same\")(critic_input)\n",
        "x = layers.LeakyReLU(0.2)(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "\n",
        "x = layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\")(x)\n",
        "x = layers.LeakyReLU(0.2)(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "\n",
        "x = layers.Conv2D(256, kernel_size=4, strides=2, padding=\"same\")(x)\n",
        "x = layers.LeakyReLU(0.2)(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "\n",
        "x = layers.Conv2D(512, kernel_size=4, strides=2, padding=\"same\")(x)\n",
        "x = layers.LeakyReLU(0.2)(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "\n",
        "x = layers.Conv2D(1, kernel_size=4, strides=1, padding=\"valid\")(x)\n",
        "x = layers.Activation(activation=\"sigmoid\")(x)\n",
        "\n",
        "critic_output = layers.Flatten()(x)\n",
        "\n",
        "critic = models.Model(critic_input, critic_output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "lH1J3-IMYHvR",
        "outputId": "96eefed5-cee4-41a4-ffe1-e5ca8a3a52a2"
      },
      "outputs": [],
      "source": [
        "critic.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fLL86yxLYPD5",
        "outputId": "2c54efae-f71e-47c1-df8d-73441a2470c8"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(critic,\n",
        "                          show_dtype=False,\n",
        "                          show_layer_activations=True,\n",
        "                          show_layer_names=False,\n",
        "                          show_shapes=True\n",
        "                          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFPnVxhtZiTL"
      },
      "source": [
        "### Build the Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5zxRCrlZlZL"
      },
      "outputs": [],
      "source": [
        "generator_input = layers.Input(shape=(LATENT_SPACE_DIMS, ))\n",
        "\n",
        "x = layers.Reshape((1,1,LATENT_SPACE_DIMS))(generator_input)\n",
        "\n",
        "x = layers.Conv2DTranspose(512, kernel_size=4, strides=1, padding=\"valid\", use_bias=False)(x)\n",
        "x = layers.BatchNormalization(momentum=0.9)(x)\n",
        "x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "# x = layers.UpSampling2D(size=2)(x)\n",
        "# x = layers.Conv2D(256, kernel_size=4, strides=1, padding=\"same\")(x)\n",
        "x = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n",
        "x = layers.BatchNormalization(momentum=0.9)(x)\n",
        "x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "x = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n",
        "x = layers.BatchNormalization(momentum=0.9)(x)\n",
        "x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "x = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding=\"same\", use_bias=False)(x)\n",
        "x = layers.BatchNormalization(momentum=0.9)(x)\n",
        "x = layers.LeakyReLU(0.2)(x)\n",
        "\n",
        "x = layers.Conv2DTranspose(CHANNELS, kernel_size=4, strides=2, padding=\"same\")(x)\n",
        "generator_output = layers.Activation(activation=\"tanh\")(x)\n",
        "\n",
        "generator = models.Model(generator_input, generator_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "Y538TsbJcWt6",
        "outputId": "f3d78c4b-33bd-4d03-93ae-2dbc9193a3bb"
      },
      "outputs": [],
      "source": [
        "generator.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-57QxoJCccGq",
        "outputId": "104bbcca-2e22-4749-ddbc-6a20b69792ce"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(generator,\n",
        "                          show_dtype=False,\n",
        "                          show_layer_activations=True,\n",
        "                          show_layer_names=False,\n",
        "                          show_shapes=True\n",
        "                          )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJTZScDGZl1i"
      },
      "source": [
        "### Build/Implement WGAN-GP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AN3sHsaQZmPA"
      },
      "outputs": [],
      "source": [
        "class WGANGP(models.Model):\n",
        "    def __init__(self, critic, generator, latent_dim, critic_steps=3, gp_weight=10.0):\n",
        "        super(WGANGP, self).__init__()\n",
        "\n",
        "        self.critic = critic\n",
        "        self.generator = generator\n",
        "        self.latent_dim = latent_dim\n",
        "        self.critic_steps = critic_steps\n",
        "        self.gp_weight = gp_weight\n",
        "\n",
        "    def compile(self, optimizer_critic, optimizer_generator):\n",
        "        super(WGANGP, self).compile()\n",
        "\n",
        "        self.optimizer_critic = optimizer_critic\n",
        "        self.optimizer_generator = optimizer_generator\n",
        "        self.loss_metric_critic = tf.keras.metrics.Mean(name=\"critic_loss\")\n",
        "        self.loss_metric_wasserstein = tf.keras.metrics.Mean(name=\"wasserstein_loss\")\n",
        "        self.loss_metric_gp = tf.keras.metrics.Mean(name=\"gradient_penalty_loss\")\n",
        "        self.loss_metric_generator = tf.keras.metrics.Mean(name=\"generator_loss\")\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.loss_metric_critic, self.loss_metric_wasserstein, self.loss_metric_gp, self.loss_metric_generator]\n",
        "\n",
        "    def train_step(self, real_images):\n",
        "        batch_size = tf.shape(real_images)[0]\n",
        "\n",
        "        for i in range(self.critic_steps):\n",
        "\n",
        "            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                fake_images = self.generator(random_latent_vectors, training=True)\n",
        "\n",
        "                fake_predictions = self.critic(fake_images, training=True)\n",
        "                real_predictions = self.critic(real_images, training=True)\n",
        "\n",
        "                loss_wasserstein = tf.reduce_mean(fake_predictions) - tf.reduce_mean(real_predictions)\n",
        "                loss_gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
        "                loss_critic = loss_wasserstein + loss_gp*self.gp_weight\n",
        "            \n",
        "            gradient_critic = tape.gradient(loss_critic, self.critic.trainable_variables)\n",
        "            self.optimizer_critic.apply_gradients(zip(gradient_critic, self.critic.trainable_variables)) \n",
        "\n",
        "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_images = self.generator(random_latent_vectors)\n",
        "\n",
        "            fake_predictions = self.critic(fake_images, training=True)\n",
        "            loss_generator = -1* tf.reduce_mean(fake_predictions)\n",
        "\n",
        "        gradient_generator = tape.gradient(loss_generator, self.generator.trainable_variables)\n",
        "        self.optimizer_generator.apply_gradients(zip(gradient_generator, self.generator.trainable_variables ))\n",
        "            \n",
        "        # update loss metrics\n",
        "        self.loss_metric_critic.update_state(loss_critic)\n",
        "        self.loss_metric_wasserstein.update_state(loss_wasserstein)\n",
        "        self.loss_metric_gp.update_state(loss_gp)\n",
        "        self.loss_metric_generator.update_state(loss_generator)\n",
        "\n",
        "        return {cur_metric.name:cur_metric.result() for cur_metric in self.metrics}\n",
        "\n",
        "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
        "        alpha = tf.random.normal([batch_size, 1, 1, 1, 1], 0.0, 1.0)\n",
        "        diff = fake_images - real_images\n",
        "        interpolated = real_images + alpha*diff\n",
        "\n",
        "        with tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(interpolated)\n",
        "            pred = self.critic(interpolated, training=True)\n",
        "\n",
        "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "        norm = tf.sqrt( tf.reduce_sum( tf.square(grads), axis=[1,2,3] ) )    # L2 norm, gradient length\n",
        "        gp = tf.reduce_mean((norm-1.0)**2)    # mean squared distance between L2 norm and 1\n",
        "        return gp\n",
        "\n",
        "\n",
        "    def generate_image(self, amount=1, fix_size_if_amount_1=True):\n",
        "        self.generator.predict(\n",
        "                        tf.random.normal(shape=(amount, self.latent_dim))\n",
        "                        )\n",
        "\n",
        "        # postprocessing: reverse normalization\n",
        "        generation = generation*(255//2) + (255//2)\n",
        "\n",
        "        if not isinstance(generation, np.ndarray):\n",
        "            generation = generation.numpy()\n",
        "\n",
        "        # fix the shape if needed\n",
        "        if len(generation.shape) == 4 and amount==1 and fix_size_if_amount_1:\n",
        "            generation = np.reshape(generation, generation.shape[1:])\n",
        "\n",
        "        return generation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NUWDYR3MMCP"
      },
      "source": [
        "Add some Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtK1UU3zMGNf"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"./checkpoint/checkpoint.keras\",\n",
        "    save_weights_only=False,\n",
        "    save_freq=\"epoch\",\n",
        "    verbose=0,\n",
        ")\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"./logs\")\n",
        "\n",
        "\n",
        "class ImageGenerator(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, num_img, latent_dim):\n",
        "        self.num_img = num_img\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        random_latent_vectors = tf.random.normal(\n",
        "            shape=(self.num_img, self.latent_dim)\n",
        "        )\n",
        "        generated_images = self.model.generator(random_latent_vectors)\n",
        "        generated_images = generated_images * 127.5 + 127.5\n",
        "        generated_images = generated_images.numpy()\n",
        "        imshow(\n",
        "            generated_images,\n",
        "            cols=self.num_img//2,\n",
        "            save_to=f\"./output/generated_img_{epoch:03}.png\",\n",
        "            color_space=\"gray\",\n",
        "            invert=True,\n",
        "            title=f\"Generations on Epoch: {epoch:03d}\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jQonixQMJHWF",
        "outputId": "995649e7-0b7c-4da0-b295-d3ec7e2f1efc"
      },
      "outputs": [],
      "source": [
        "wgangp = WGANGP(critic=critic,\n",
        "              generator=generator,\n",
        "              latent_dim=LATENT_SPACE_DIMS,\n",
        "              critic_steps=CRITIC_STEPS,\n",
        "              gp_weight=GP_WEIGHT\n",
        "              )\n",
        "\n",
        "wgangp.compile(\n",
        "    optimizer_discriminator=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_DIS, beta_1=0.5, beta_2=0.999),\n",
        "    optimizer_generator=tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE_GEN, beta_1=0.5, beta_2=0.999)\n",
        ")\n",
        "\n",
        "wgangp.fit(\n",
        "    train_data,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[\n",
        "        model_checkpoint_callback,\n",
        "        tensorboard_callback,\n",
        "        ImageGenerator(num_img=10, latent_dim=LATENT_SPACE_DIMS),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwykB3S1hYZa"
      },
      "source": [
        "### Evaluation\n",
        "\n",
        "Watch the train images, does it improve from each epoch?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generation = wgangp.generate_image(amount=10)\n",
        "imshow(generation,\n",
        "        cols=5,\n",
        "        invert=True\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBNr9WHFQ0Mk"
      },
      "source": [
        "**Originality**\n",
        "\n",
        "One key factor of a \"good\"/\"successfull\" generative model is to create different new images and not too similiar to the train data.\n",
        "\n",
        "To test this we can use a image similarity metric like a simple L1 distance, where we compute the pixels and build the mean of these differences.\n",
        "\n",
        "- Best-Score: 255.0\n",
        "- Worst-Score: 0.0    (no difference)\n",
        "\n",
        "> Make sure that the value areas are the same (scaling is same) + the values are same inverted => you can analyze it visualy to check if the similarity is right!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57-A-BkbhYvF"
      },
      "outputs": [],
      "source": [
        "def image_similarity(n_test_runs, model, train_data, random_dropout=0.8):\n",
        "    total_originality = []\n",
        "\n",
        "    # run n experiments/generations\n",
        "    for _ in range(n_test_runs):\n",
        "        # generate an image\n",
        "        generation = model.generate_image()\n",
        "\n",
        "        # compute a originality, is the image new in comparison ot the whole train dataset?\n",
        "        originality = []\n",
        "        for cur_images in train_data:\n",
        "            cur_images = cur_images.numpy()\n",
        "\n",
        "            for cur_image in cur_images:\n",
        "                if np.random.random() > random_dropout:\n",
        "                    originality += [np.mean(np.abs(generation - cur_image))]\n",
        "\n",
        "        total_originality = [np.mean(np.array(originality))]\n",
        "\n",
        "    total_originality_arr = np.array(total_originality)\n",
        "    total_originality_mean = np.mean(total_originality_arr)\n",
        "    total_originality_var = np.var(total_originality_arr)\n",
        "\n",
        "    print(f\"\\n{'-'*32}\\nResult (smaller = better:)\")\n",
        "    print(f\"Mean Originality: {total_originality_mean}\")\n",
        "    print(f\"Variance Originality: {total_originality_var}\")\n",
        "\n",
        "    return total_originality_mean, total_originality_var\n",
        "\n",
        "\n",
        "\n",
        "total_originality_mean, total_originality_var = image_similarity(\n",
        "                                                    n_test_runs=50,\n",
        "                                                    model=wgangp,\n",
        "                                                    train_data=train_data,\n",
        "                                                    random_dropout=0.9\n",
        "                                                )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLMW-v0pUS3w"
      },
      "source": [
        "check similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v3yJLgFXQbt"
      },
      "outputs": [],
      "source": [
        "from IPython.display import clear_output\n",
        "\n",
        "def manually_check_similarity(model, train_data):\n",
        "    batches = len(train_data)\n",
        "    train_data_gen = iter(train_data)\n",
        "\n",
        "    # generate an image\n",
        "    generation = model.generate_image()\n",
        "\n",
        "    # grap 5 samples\n",
        "    # random_batch_shuffle = np.random.randint(1, batches*next(iter(train_data)).numpy().shape[0])\n",
        "    random_batch_shuffle = np.random.randint(1, batches)\n",
        "\n",
        "    for _ in range(random_batch_shuffle):\n",
        "        try:\n",
        "            cur_batch = next(train_data_gen)\n",
        "        except Exception:    # OutOfRangeError\n",
        "            train_data_gen = iter(train_data)\n",
        "            cur_batch = next(train_data_gen)\n",
        "\n",
        "    cur_batch = cur_batch.numpy()\n",
        "    indices = np.random.choice(cur_batch.shape[0], size=5, replace=False)\n",
        "    cur_batch = cur_batch[indices]\n",
        "    title = []\n",
        "\n",
        "    for cur_image in cur_batch:\n",
        "        originality = np.mean(np.abs(generation - cur_image))\n",
        "        title += [f\"real, sim: {originality:.2f}\"]    # :e -> scientific notation\n",
        "\n",
        "    generation = np.reshape(generation, (1,)+ generation.shape)\n",
        "    imshow(np.concatenate((generation, generation, generation, generation, generation, cur_batch), axis=0),\n",
        "                        cols=5,\n",
        "                        title=[\"generated images\"]*5+title,\n",
        "                        invert=True\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "print(\"Compare one generated image to diferent images, are the similarity values correct?\\n    (Run this cell again to get other images)\\n\")\n",
        "manually_check_similarity(model=wgangp, train_data=train_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6mWd_XTmmR-"
      },
      "source": [
        "Are there missing postprocessings?\n",
        "- Scaling right?\n",
        "- Normalization reversed of the generated images?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQngb7qqklHT"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
