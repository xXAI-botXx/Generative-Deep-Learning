{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "Long Short-Term Memory Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hardware_info(use_in_notebook=True, install_packages=True):\n",
    "    import sys\n",
    "    import subprocess\n",
    "    import importlib.util\n",
    "    \n",
    "    if install_packages:\n",
    "        if importlib.util.find_spec(\"psutil\") is None:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"psutil\"], check=True)\n",
    "        if importlib.util.find_spec(\"gputil\") is None:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"gputil\"], check=True)\n",
    "        if importlib.util.find_spec(\"py-cpuinfo\") is None:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"py-cpuinfo\"], check=True)\n",
    "\n",
    "    # import needed packages\n",
    "    import platform\n",
    "    import psutil\n",
    "    import GPUtil\n",
    "    from cpuinfo import get_cpu_info\n",
    "\n",
    "    if use_in_notebook:\n",
    "        if install_packages and importlib.util.find_spec(\"ipython\") is None:\n",
    "            subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"ipython\"], check=True)\n",
    "\n",
    "        from IPython.display import clear_output\n",
    "        clear_output()\n",
    "    else:\n",
    "        pass\n",
    "        # os.system('cls' if os.name == 'nt' else 'clear')\n",
    "\n",
    "    print(\"-\"*32, \"\\nYour Hardware:\\n\")\n",
    "\n",
    "    # General\n",
    "    print(\"    ---> General <---\")\n",
    "    print(\"Operatingsystem:\", platform.system())\n",
    "    print(\"Version:\", platform.version())\n",
    "    print(\"Architecture:\", platform.architecture())\n",
    "    print(\"Processor:\", platform.processor())\n",
    "\n",
    "    # GPU-Information\n",
    "    print(\"\\n    ---> GPU <---\")\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    for gpu in gpus:\n",
    "        print(\"GPU Name:\", gpu.name)\n",
    "        print(\"VRAM Total:\", gpu.memoryTotal, \"MB\")\n",
    "        print(\"VRAM Used:\", gpu.memoryUsed, \"MB\")\n",
    "        print(\"Utilization:\", gpu.load * 100, \"%\")\n",
    "\n",
    "    # CPU-Information\n",
    "    print(\"\\n    ---> CPU <---\")\n",
    "    cpu_info = get_cpu_info()\n",
    "    print(\"CPU-Name:\", cpu_info[\"brand_raw\"])\n",
    "    print(\"CPU Kernels:\", psutil.cpu_count(logical=False))\n",
    "    print(\"Logical CPU-Kernels:\", psutil.cpu_count(logical=True))\n",
    "    print(\"CPU-Frequence:\", psutil.cpu_freq().max, \"MHz\")\n",
    "    print(\"CPU-Utilization:\", psutil.cpu_percent(interval=1), \"%\")\n",
    "\n",
    "    # RAM-Information\n",
    "    print(\"\\n    ---> RAM <---\")\n",
    "    ram = psutil.virtual_memory()\n",
    "    print(\"RAM Total:\", ram.total // (1024**3), \"GB\")\n",
    "    print(\"RAM Available:\", ram.available // (1024**3), \"GB\")\n",
    "    print(\"RAM-Utilization:\", ram.percent, \"%\")\n",
    "\n",
    "    print(f\"\\n{'-'*32}\")\n",
    "\n",
    "\n",
    "\n",
    "get_hardware_info(use_in_notebook=True, install_packages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cur_date_time_as_str():\n",
    "    now = datetime.now()\n",
    "    return f\"{now.year:04}-{now.month:02}-{now.day:02}_{now.hour:02}-{now.minute:02}-{now.second:02}\"\n",
    "\n",
    "get_cur_date_time_as_str()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "def load_kaggle_dataset(author_name, dataset_name, on_google_colab, \n",
    "                        download_path, goal_path,\n",
    "                        kaggle_local_path=\"./\", kaggle_file_name=\"kaggle.json\"):\n",
    "    \"\"\"\n",
    "    Example:\n",
    "\n",
    "    >>> load_kaggle_dataset(\n",
    "            author_name=\"joosthazelzet\", \n",
    "            dataset_name=\"lego-brick-images\",\n",
    "            on_google_colab=True,            \n",
    "            download_path=\"/content/\", \n",
    "            goal_path=f\"/content/dataset\",         \n",
    "            kaggle_local_path=\"./\", \n",
    "            kaggle_file_name=\"kaggle.json\")\n",
    "    \"\"\"\n",
    "    # variables\n",
    "    print(\"Set some variables...\")\n",
    "    dataset_download_name = f\"{author_name}/{dataset_name}\"\n",
    "   \n",
    "    zip_file_name = f\"{dataset_name}.zip\"\n",
    "    zip_file_download_path = os.path.join(download_path, zip_file_name)\n",
    "\n",
    "    kaggle_file_cur_path = os.path.join(kaggle_local_path, kaggle_file_name)\n",
    "    kaggle_goal_path = os.path.expanduser(\"~/.kaggle\") if platform.system().lower() == \"windows\" else \"/root/.config/kaggle\"\n",
    "    kaggle_goal_file_path = os.path.join(kaggle_goal_path, kaggle_file_name)\n",
    "\n",
    "    # make sure that the goal path exist\n",
    "    os.makedirs(goal_path, exist_ok=True)\n",
    "    os.makedirs(kaggle_goal_path, exist_ok=True)\n",
    "\n",
    "    print(\"Finding and placing the API file...\")\n",
    "    # upload in google colab\n",
    "    if on_google_colab:\n",
    "        kaggle_local_path = \"./\"\n",
    "        kaggle_file_cur_path = os.path.join(kaggle_local_path, kaggle_file_name)\n",
    "        if os.path.exists(kaggle_file_cur_path):\n",
    "            os.remove(kaggle_file_cur_path)\n",
    "\n",
    "        from google.colab import files\n",
    "        files.upload()  # choose your local 'kaggle.json' file\n",
    "\n",
    "    # get the kaggle API file to the right spot\n",
    "    if os.path.exists(kaggle_goal_file_path):\n",
    "        os.remove(kaggle_goal_file_path)\n",
    "    shutil.copy2(kaggle_file_cur_path, kaggle_goal_path)\n",
    "    os.chmod(kaggle_goal_file_path, 600)    # set right rights\n",
    "    print(f\"Cpopied to: {kaggle_goal_path}\")\n",
    "\n",
    "    # init Kaggle API\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "    print(\"Autheticating at Kaggle API...\")\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "\n",
    "    # make sure the file not exist already\n",
    "    if os.path.exists(zip_file_download_path):\n",
    "        os.remove(zip_file_download_path)\n",
    "\n",
    "    # download kaggle dataset\n",
    "    print(\"Downloading dataset...\")\n",
    "    #    -> dataset name just in the https link the last 2 items\n",
    "    # !kaggle datasets download -d joosthazelzet/lego-brick-images\n",
    "    api.dataset_download_files(dataset_download_name, path=download_path, unzip=False)\n",
    "\n",
    "    # Unzip the downloaded dataset\n",
    "    print(\"Unzipping dataset...\")\n",
    "    if os.path.exists(goal_path):\n",
    "        shutil.rmtree(goal_path)\n",
    "    # !unzip -q \"lego-brick-images.zip\" -d dataset\n",
    "    with zipfile.ZipFile(zip_file_download_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(goal_path)\n",
    "\n",
    "    # delete zip file\n",
    "    os.remove(zip_file_download_path)\n",
    "\n",
    "    print(f\"Congratulations! Downloaded successfull '{dataset_name}' from '{author_name}' ðŸ¥³ðŸ˜Ž\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = get_cur_date_time_as_str() + \"_DCGAN\"\n",
    "LOG_DIR = \"./logs/fit/\" + get_cur_date_time_as_str()\n",
    "\n",
    "EMBEDDING_DIMS = 100\n",
    "CHANNELS = 1\n",
    "EPOCHS = 300\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "\n",
    "os.makedirs(f\"./logs\", exist_ok=True)\n",
    "os.makedirs(f\"./checkpoints\", exist_ok=True)\n",
    "os.makedirs(f\"./models/{EXPERIMENT_NAME}\", exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "os.makedirs(\"./output/\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading & Prep\n",
    "\n",
    "The Epicurious Recipes dataset contains over 200.000 recipes.\n",
    "\n",
    "See: https://www.kaggle.com/datasets/hugodarwood/epirecipes\n",
    "\n",
    "\n",
    "Getting a Kaggle API Key:\n",
    "1. Go to https://www.kaggle.com/ and create or sign in your account\n",
    "2. Click on your profile picture > Settings and go to API\n",
    "3. Click on 'Create New Token' and the 'kaggle.json' fie should download automatically\n",
    "4. Now you can use this method to load this json file and download the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_kaggle_dataset(\n",
    "    author_name=\"hugodarwood\",\n",
    "    dataset_name=\"epirecipes\",\n",
    "    on_google_colab=True,\n",
    "    download_path=\"/content/\",\n",
    "    goal_path=f\"/content/dataset\",\n",
    "    kaggle_local_path=\"./\",\n",
    "    kaggle_file_name=\"kaggle.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./dataset/full_format_recipes.json\", \"r\") as json_file:\n",
    "    recipe_dataset = json.load(json_file)\n",
    "\n",
    "filtered_data = [\n",
    "    f\"Recipe for {x['title']} |\".join(x['directions'])\n",
    "    for x in recipe_dataset\n",
    "    if 'title' in x\n",
    "    and x['title'] is not None\n",
    "    and 'directions' in x\n",
    "    and x['directions'] is not None\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_punctuation(s):\n",
    "    s = re.sub(f\"([{string.punctuation}])\", r'\\1', s)\n",
    "    s = re.sub('+', '', s)\n",
    "    return s\n",
    "\n",
    "text_data = [pad_punctuation(x for x in filtered_data)]\n",
    "\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(text_data.batch(32).shuffle(1000))\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize='lower',\n",
    "    max_tokens=10000,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=200+1\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_inputs(text):\n",
    "    text = tf.expand(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "train_ds  = text_ds.map(prep_inputs)\n",
    "\n",
    "print(type(train_ds))\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape(None,), dtype=\"int32\")\n",
    "\n",
    "x = layers.Embedding(10000, EMBEDDING_DIMS)(inputs)\n",
    "\n",
    "x = layers.LSTM(128, return_sequence=True)(x)\n",
    "\n",
    "outputs = layers.Dense(10000, activation='softmax')(x)\n",
    "\n",
    "lstm = models.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, index_to_word, top_k=10):\n",
    "        self.index_to_word = index_to_word\n",
    "        self.word_to_index = {#\n",
    "            word:index for index, word in enumerate(index_to_word)\n",
    "        }\n",
    "\n",
    "    def sample_from(self, probs, temperature):\n",
    "        probs = probs**(1/temperature)\n",
    "        probs = probs / np.sum(probs)\n",
    "        return np.random.choice(len(probs), p=probs), probs\n",
    "\n",
    "    def generate(self, start_prompt, max_tokens, temeprature):\n",
    "        start_tokens = [\n",
    "            self.word_to_index(x, 1) for x in start_prompt.split()\n",
    "        ]\n",
    "        sample_token = None\n",
    "        info = []\n",
    "\n",
    "        while len(start_tokens)<max_tokens and sample_token!=0:\n",
    "            x = np.array([start_tokens])\n",
    "            y = self.model.predict(x)\n",
    "            sample_token, probs = self.sample_from(y[0][-1], temeprature)\n",
    "            \n",
    "            info += [f\"prompt: {start_prompt} word_probs: {probs}\"]\n",
    "\n",
    "            start_tokens += [sample_token]\n",
    "            start_prompt = start_prompt + '' + self.index_to_word[sample_token]\n",
    "\n",
    "        print(f\"\\nGenerated Text:\\n'{start_prompt}'\\n\")\n",
    "        return info\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.generate(\"recipes for\", max_tokens=100, temeprature=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_generator = TextGenerator(vocab)\n",
    "\n",
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "lstm.compile(\"adam\", loss_func)\n",
    "lstm.fit(\n",
    "    train_ds, \n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[text_generator])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
